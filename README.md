# ğŸ§  Seq2Seq Transliteration with Attention â€” Aksharantar (IIT Madras)

**Romanized â†’ Devanagari transliteration** using a character-level Seq2Seq model with **LSTM + Bahdanau Attention**.  
This repo is a compact, well-documented, and reproducible solution prepared for the IIT Madras Technical Aptitude challenge.

---

## ğŸ” Quick summary
- **Task:** Map romanized character sequences (e.g., `ghar`) â†’ native script (e.g., `à¤˜à¤°`).  
- **Model:** Encoder (LSTM) + Decoder (LSTM) with Bahdanau additive attention.  
- **Language:** Python + PyTorch. Ready to run on Colab GPU.

---

## ğŸ“ Repo structure (what's important)
```
Seq2Seq-Aksharantar-IITM/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ encoder.py
â”‚   â”‚   â”œâ”€â”€ decoder.py
â”‚   â”‚   â”œâ”€â”€ attention.py
â”‚   â”‚   â””â”€â”€ seq2seq.py
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â”œâ”€â”€ tokenizer.py
â”‚   â”‚   â””â”€â”€ dataset.py
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ metrics.py
â”‚       â””â”€â”€ training_loop.py
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ train.py
â”‚   â””â”€â”€ evaluate.py
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ model_config.yaml
â”‚   â””â”€â”€ data_config.yaml
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ Transliteration_Report.ipynb
â”œâ”€â”€ data/
â”‚   â””â”€â”€ raw/sample.tsv
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## ğŸ§© Architecture (ASCII diagram)
```
      Input (Romanized) chars
               â”‚
         [Embedding Layer]
               â”‚
         [LSTM Encoder]  -> encoder outputs (sequence of h vectors)
               â”‚
               â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
               â”‚                   â–¼
            (enc outputs)    Bahdanau Attention
               â”‚                   â”‚
               â”‚                   â–¼
            (context vector) â”€â”€> [LSTM Decoder] (uses prev token + context)
                                     â”‚
                                  [Linear]
                                     â”‚
                                  Softmax
                                     â”‚
                                  Output char
```

---

## âš™ï¸ Default Configs (chosen for Colab & IITM)
- `embedding_size = 128`  
- `hidden_size = 256`  
- `rnn_cell = LSTM`  
- `use_attention = true`  
- `num_layers = 1`  
- `dropout = 0.2`  
- `batch_size = 64`  
- `learning_rate = 0.001`

You can change these in `config/model_config.yaml`.

---

## ğŸ§® Math (Answer to assignment questions â€” explicit & worked example)

### Notation
- `e` = embedding dimension  
- `h` = hidden dimension (encoder & decoder)  
- `T` = input/output sequence length (assumed equal for derivation)  
- `V` = vocabulary size (same for source & target)  
- Single-layer encoder and decoder (adjust multipliers for multiple layers)

### Parameter count (vanilla RNN symbolic)
\[
P_\text{total} = 2Ve + 2(eh + h^2 + h) + hV + V
\]
(Embeddings + encoder RNN + decoder RNN + output projection)

### Computation (dominant matmuls) â€” forward pass
\[
\text{Matmuls} = T \cdot (2eh + 2h^2 + hV)
\]
Multiply by 2 for multiply+add FLOPs. Training (~backprop) â‰ˆ Ã—3 forward cost.

### Worked numeric example
Use `e=128`, `h=256`, `V=5000`, `T=20`:

- Parameters â‰ˆ **2,762,120** (â‰ˆ 10.5 MB in float32)  
- Forward matmuls â‰ˆ **29,532,160** â†’ â‰ˆ **59M FLOPs** (multiply+add)

(Full derivation and LSTM/GRU variants are in `notebooks/Transliteration_Report.ipynb`.)

---

## ğŸš€ How to run (quick)
1. Install requirements:
```bash
pip install -r requirements.txt
```

2. Quick smoke test (uses tiny sample shipped in `data/raw/sample.tsv`):
```bash
python scripts/train.py --config config/model_config.yaml --quick_test
```

3. Full training:
```bash
python scripts/train.py --config config/model_config.yaml --out_dir checkpoints
```

4. Evaluate a checkpoint:
```bash
python scripts/evaluate.py --config config/model_config.yaml --checkpoint checkpoints/best.pt
```

---

## ğŸ“Š Notebook & Visualizations
Open `notebooks/Transliteration_Report.ipynb` to see:
- Architecture explanation
- Mathematical derivation (step-by-step)
- Attention visualization sketch (heatmaps) and sample outputs

---

## ğŸ§¾ References & Acknowledgements
- AI4Bharat â€” Aksharantar dataset  
- Bahdanau et al. (2014) â€” Neural Machine Translation by Jointly Learning to Align and Translate  
- PyTorch Seq2Seq tutorial

---

## ğŸ‘©â€ğŸ’» Author
Prepared for IIT Madras Technical Aptitude Challenge by **Navyashree N**.

---