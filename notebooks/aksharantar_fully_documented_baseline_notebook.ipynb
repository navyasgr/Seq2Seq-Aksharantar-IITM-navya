{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c960aa43",
   "metadata": {},
   "source": [
    "# Aksharantar — Fully Documented Baseline Notebook\n",
    "\n",
    "Character-level seq2seq transliteration (Roman → Devanagari).\n",
    "\n",
    "This notebook is organized for reviewers: clear explanations, runnable code cells, and baseline-only implementation (1-layer LSTM encoder + 1-layer LSTM decoder)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75879298",
   "metadata": {},
   "source": [
    "\n",
    "## Problem statement & Assumptions\n",
    "\n",
    "**Task:** Map romanized strings (Latin characters) to Devanagari characters using a character-level seq2seq model.\n",
    "\n",
    "**Baseline constraints (must satisfy):**\n",
    "- 1-layer encoder + 1-layer decoder\n",
    "- LSTM cell (allowed)\n",
    "- Teacher forcing during training\n",
    "- Save best baseline checkpoint at `results/checkpoints/best_baseline.pt`\n",
    "\n",
    "**Assumptions for reproducibility:**\n",
    "- `data/ready/hin_data.npz` exists and contains `X_train, Y_train, X_val, Y_val, X_test, Y_test` numpy arrays.\n",
    "- `data/processed/hin_clean.csv` exists for building vocabularies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea371a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ================= CONFIG (baseline default) =================\n",
    "import os\n",
    "os.makedirs(\"results/checkpoints\", exist_ok=True)\n",
    "\n",
    "# Baseline (required)\n",
    "BASELINE = True        # set False only when running extension (do NOT do this for baseline submission)\n",
    "RNN_CELL = \"LSTM\"      # allowed: \"RNN\",\"GRU\",\"LSTM\"\n",
    "NUM_LAYERS = 1         # baseline = 1 (encoder+decoder)\n",
    "EMB_DIM = 256\n",
    "HID_DIM = 512\n",
    "\n",
    "# Training\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 20\n",
    "LR = 1e-3\n",
    "TEACHER_FORCING_START = 0.5\n",
    "TEACHER_FORCING_END = 0.1\n",
    "CLIP = 1.0\n",
    "\n",
    "CHECKPOINT_BASELINE = \"results/checkpoints/best_baseline.pt\"\n",
    "\n",
    "# Device\n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "# Load prepared data and vocabs if not in memory\n",
    "import numpy as np, pandas as pd\n",
    "if 'X_train' not in globals():\n",
    "    npz = np.load(\"data/ready/hin_data.npz\", allow_pickle=True)\n",
    "    X_train, Y_train = npz[\"X_train\"], npz[\"Y_train\"]\n",
    "    X_val, Y_val = npz[\"X_val\"], npz[\"Y_val\"]\n",
    "    X_test, Y_test = npz[\"X_test\"], npz[\"Y_test\"]\n",
    "\n",
    "if 'roman2idx' not in globals():\n",
    "    df = pd.read_csv(\"data/processed/hin_clean.csv\", names=[\"roman\",\"devanagari\"])\n",
    "    roman_chars = sorted(list(set(\"\".join(df[\"roman\"].values))))\n",
    "    dev_chars = sorted(list(set(\"\".join(df[\"devanagari\"].values))))\n",
    "    special_tokens = [\"<pad>\",\"<sos>\",\"<eos>\"]\n",
    "    roman_vocab = special_tokens + roman_chars\n",
    "    dev_vocab = special_tokens + dev_chars\n",
    "    roman2idx = {c:i for i,c in enumerate(roman_vocab)}\n",
    "    idx2roman = {i:c for c,i in roman2idx.items()}\n",
    "    dev2idx = {c:i for i,c in enumerate(dev_vocab)}\n",
    "    idx2dev = {i:c for c,i in dev2idx.items()}\n",
    "\n",
    "INPUT_DIM = len(roman2idx)\n",
    "OUTPUT_DIM = len(dev2idx)\n",
    "print(\"INPUT_DIM:\", INPUT_DIM, \"OUTPUT_DIM:\", OUTPUT_DIM)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd81b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "\n",
    "class CharDataset(Dataset):\n",
    "    \"\"\"Simple dataset wrapper. Stores integer-encoded sequences (padded).\n",
    "\n",
    "    X and Y are expected to be numpy arrays with shapes (N, T).\n",
    "    \"\"\"\n",
    "    def __init__(self, X, Y):\n",
    "        self.X = torch.tensor(X, dtype=torch.long)\n",
    "        self.Y = torch.tensor(Y, dtype=torch.long)\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.Y[idx]\n",
    "\n",
    "train_loader = DataLoader(CharDataset(X_train, Y_train), batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(CharDataset(X_val, Y_val), batch_size=BATCH_SIZE)\n",
    "test_loader = DataLoader(CharDataset(X_test, Y_test), batch_size=BATCH_SIZE)\n",
    "\n",
    "print(\"Data loaders ready. Train size:\", len(train_loader.dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05360641",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    \"\"\"Encoder: embedding -> single-layer LSTM (baseline).\"\"\"\n",
    "    def __init__(self, input_dim, emb_dim, hid_dim, n_layers=1, cell=\"LSTM\", dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim, padding_idx=0)\n",
    "        self.cell = cell.upper()\n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_layers = n_layers\n",
    "        if self.cell == \"LSTM\":\n",
    "            self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, batch_first=True, dropout=dropout)\n",
    "        elif self.cell == \"GRU\":\n",
    "            self.rnn = nn.GRU(emb_dim, hid_dim, n_layers, batch_first=True, dropout=dropout)\n",
    "        else:\n",
    "            self.rnn = nn.RNN(emb_dim, hid_dim, n_layers, batch_first=True, nonlinearity='tanh', dropout=dropout)\n",
    "\n",
    "    def forward(self, src):\n",
    "        # src: (batch, src_len)\n",
    "        emb = self.embedding(src)                  # (batch, src_len, emb_dim)\n",
    "        outputs, hidden = self.rnn(emb)           # outputs: (batch, src_len, hid_dim)\n",
    "        return outputs, hidden\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    \"\"\"Decoder: single-step decoder. Returns raw logits for softmax.\n",
    "    For baseline we DO NOT use attention.\n",
    "    \"\"\"\n",
    "    def __init__(self, output_dim, emb_dim, hid_dim, n_layers=1, cell=\"LSTM\", dropout=0.0, attention=None, enc_hid_dim=None):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim, padding_idx=0)\n",
    "        self.cell = cell.upper()\n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.attention = attention\n",
    "        rnn_input_dim = emb_dim\n",
    "        if self.cell == \"LSTM\":\n",
    "            self.rnn = nn.LSTM(rnn_input_dim, hid_dim, n_layers, batch_first=True, dropout=dropout)\n",
    "        elif self.cell == \"GRU\":\n",
    "            self.rnn = nn.GRU(rnn_input_dim, hid_dim, n_layers, batch_first=True, dropout=dropout)\n",
    "        else:\n",
    "            self.rnn = nn.RNN(rnn_input_dim, hid_dim, n_layers, batch_first=True, nonlinearity='tanh', dropout=dropout)\n",
    "        self.fc_out = nn.Linear(hid_dim, output_dim)\n",
    "\n",
    "    def forward(self, input_token, hidden, enc_outputs=None):\n",
    "        # input_token: (batch,) dtype long (single timestep tokens)\n",
    "        emb = self.embedding(input_token).unsqueeze(1)    # (batch,1,emb)\n",
    "        out, hidden = self.rnn(emb, hidden)\n",
    "        out = out.squeeze(1)                              # (batch, hid_dim)\n",
    "        pred = self.fc_out(out)                           # (batch, output_dim)\n",
    "        return pred, hidden\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    \"\"\"Wrapper: runs encoder then auto-regressive decoder with teacher forcing option.\"\"\"\n",
    "    def __init__(self, encoder, decoder, device, teacher_forcing_ratio=0.5):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        self.teacher_forcing_ratio = teacher_forcing_ratio\n",
    "\n",
    "    def forward(self, src, trg=None, max_len=None):\n",
    "        batch_size = src.size(0)\n",
    "        enc_outputs, enc_hidden = self.encoder(src)\n",
    "        dec_hidden = enc_hidden\n",
    "\n",
    "        if trg is not None:\n",
    "            # Training mode (use target for teacher forcing)\n",
    "            trg_len = trg.size(1)\n",
    "            out_dim = self.decoder.fc_out.out_features\n",
    "            outputs = torch.zeros(batch_size, trg_len, out_dim).to(self.device)\n",
    "            input_tok = trg[:,0]   # <sos> expected as first token\n",
    "            for t in range(1, trg_len):\n",
    "                pred, dec_hidden = self.decoder(input_tok, dec_hidden)\n",
    "                outputs[:,t,:] = pred\n",
    "                tf = torch.rand(1).item() < self.teacher_forcing_ratio\n",
    "                top1 = pred.argmax(1)\n",
    "                input_tok = trg[:,t] if tf else top1\n",
    "            return outputs\n",
    "        else:\n",
    "            # Inference (greedy) mode: requires max_len\n",
    "            assert max_len is not None\n",
    "            out_dim = self.decoder.fc_out.out_features\n",
    "            outputs = torch.zeros(batch_size, max_len, out_dim).to(self.device)\n",
    "            input_tok = torch.full((batch_size,), dev2idx['<sos>'], dtype=torch.long, device=self.device)\n",
    "            for t in range(max_len):\n",
    "                pred, dec_hidden = self.decoder(input_tok, dec_hidden)\n",
    "                outputs[:,t,:] = pred\n",
    "                input_tok = pred.argmax(1)\n",
    "            return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78bbe72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# initialize model (baseline)\n",
    "enc = Encoder(INPUT_DIM, EMB_DIM, HID_DIM, n_layers=NUM_LAYERS, cell=RNN_CELL, dropout=0.2).to(device)\n",
    "dec = Decoder(OUTPUT_DIM, EMB_DIM, HID_DIM, n_layers=NUM_LAYERS, cell=RNN_CELL, dropout=0.2).to(device)\n",
    "model = Seq2Seq(enc, dec, device, teacher_forcing_ratio=TEACHER_FORCING_START).to(device)\n",
    "\n",
    "import torch.optim as optim\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, patience=2, verbose=True)\n",
    "import torch.nn as nn\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=roman2idx['<pad>'])\n",
    "\n",
    "print('Model parameter count (trainable):', sum(p.numel() for p in model.parameters() if p.requires_grad))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963a306b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import time\n",
    "best_val = float('inf')\n",
    "no_improve = 0\n",
    "early_stop_patience = 5\n",
    "\n",
    "train_losses, val_losses = [], []\n",
    "\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    start = time.time()\n",
    "    # linear TF decay across epochs\n",
    "    frac = (epoch-1)/max(1, EPOCHS-1)\n",
    "    model.teacher_forcing_ratio = TEACHER_FORCING_START*(1-frac) + TEACHER_FORCING_END*frac\n",
    "\n",
    "    model.train()\n",
    "    total_train = 0.0\n",
    "    for src, trg in train_loader:\n",
    "        src = src.to(device);\n",
    "        trg = trg.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(src, trg)  # (batch, trg_len, out_dim)\n",
    "        out_dim = outputs.size(-1)\n",
    "        loss = criterion(outputs[:,1:,:].reshape(-1,out_dim), trg[:,1:].reshape(-1))\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), CLIP)\n",
    "        optimizer.step()\n",
    "        total_train += loss.item()\n",
    "\n",
    "    train_loss = total_train / len(train_loader)\n",
    "\n",
    "    model.eval()\n",
    "    total_val = 0.0\n",
    "    with torch.no_grad():\n",
    "        for src, trg in val_loader:\n",
    "            src = src.to(device); trg = trg.to(device)\n",
    "            outputs = model(src, trg)\n",
    "            out_dim = outputs.size(-1)\n",
    "            loss = criterion(outputs[:,1:,:].reshape(-1,out_dim), trg[:,1:].reshape(-1))\n",
    "            total_val += loss.item()\n",
    "    val_loss = total_val / len(val_loader)\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    train_losses.append(train_loss); val_losses.append(val_loss)\n",
    "\n",
    "    print(f\"Epoch {epoch}/{EPOCHS} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | TF: {model.teacher_forcing_ratio:.3f} | Time: {time.time()-start:.1f}s\")\n",
    "\n",
    "    # checkpointing (save best)\n",
    "    if val_loss + 1e-6 < best_val:\n",
    "        best_val = val_loss\n",
    "        no_improve = 0\n",
    "        torch.save({'epoch': epoch, 'model_state_dict': model.state_dict(), 'optimizer_state_dict': optimizer.state_dict(), 'val_loss': val_loss}, CHECKPOINT_BASELINE)\n",
    "        print(\"Saved new best checkpoint ->\", CHECKPOINT_BASELINE)\n",
    "    else:\n",
    "        no_improve += 1\n",
    "        if no_improve >= early_stop_patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "print(\"Training finished. Best val:\", best_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300e060a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# decode indices to readable string\n",
    "def decode_indices(indices, idx2map):\n",
    "    s=[]\n",
    "    for i in indices:\n",
    "        ch = idx2map.get(int(i), \"\")\n",
    "        if ch in (\"<pad>\",\"<sos>\",\"<eos>\"): continue\n",
    "        s.append(ch)\n",
    "    return \"\".join(s)\n",
    "\n",
    "# greedy transliterate a single word\n",
    "def greedy_transliterate(model, word, max_len=40):\n",
    "    inp_len = X_train.shape[1]\n",
    "    seq = [roman2idx.get(ch, roman2idx['<pad>']) for ch in word]\n",
    "    seq = seq[:inp_len] + [roman2idx['<pad>']]*max(0, inp_len - len(seq))\n",
    "    src = torch.tensor([seq], dtype=torch.long).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(src, trg=None, max_len=max_len)\n",
    "    preds = outputs.argmax(-1).squeeze(0).cpu().tolist()\n",
    "    return decode_indices(preds, idx2dev)\n",
    "\n",
    "# Quick qualitative test\n",
    "import random\n",
    "for i in random.sample(range(len(X_test)), 10):\n",
    "    inp = ''.join([idx2roman[c] for c in X_test[i] if idx2roman[c] not in (\"<pad>\",)])\n",
    "    pred = greedy_transliterate(model, inp)\n",
    "    true = decode_indices([int(x) for x in Y_test[i]], idx2dev)\n",
    "    print(f\"Input: {inp} | Pred: {pred} | True: {true}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d6919c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Numeric analysis cell (params & MACs)\n",
    "d = EMB_DIM\n",
    "h = HID_DIM\n",
    "T = X_train.shape[1]\n",
    "V = max(INPUT_DIM, OUTPUT_DIM)\n",
    "\n",
    "def lstm_params(d,h,V):\n",
    "    p_layer = 4*(h*d + h*h + h)\n",
    "    total = 2*p_layer + 2*(V*d) + (V*h + V)\n",
    "    return p_layer, total\n",
    "\n",
    "def lstm_macs(d,h,T):\n",
    "    return 8 * T * h * (d + h)\n",
    "\n",
    "p_lstm_layer, P_lstm_total = lstm_params(d,h,V)\n",
    "macs_lstm = lstm_macs(d,h,T)\n",
    "\n",
    "print(\"Baseline numeric (LSTM 1-layer enc+dec):\")\n",
    "print(f\"d={d}, h={h}, T={T}, V={V}\")\n",
    "print(f\"LSTM per-layer params = {p_lstm_layer:,}\")\n",
    "print(f\"LSTM total params (enc+dec+emb+out) = {P_lstm_total:,}\")\n",
    "print(f\"LSTM forward-pass MACs (enc+dec) ≈ {macs_lstm:,}\")\n",
    "print(\"Note: backward ≈ 2-3x forward for training.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea729b8",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
