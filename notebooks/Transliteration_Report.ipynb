{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Transliteration Project — Report\n\nThis notebook documents the transliteration seq2seq project (Romanized → Devanagari). It contains:\n- Implementation notes and architecture\n- Full math derivation answering the assignment questions (parameters and computations)\n- Minimal code snippets to run the pipeline and visualize attention\n- Remarks, results, and next steps\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Architecture Overview\n\nWe implement a character-level Seq2Seq model with an RNN-based encoder and decoder. The decoder can optionally use Bahdanau additive attention. The key components in `src/models/` are:\n- `encoder.py` — flexible RNN/GRU/LSTM encoder with embedding and packed sequences\n- `decoder.py` — flexible decoder; supports optional Bahdanau attention\n- `attention.py` — Bahdanau attention module\n- `seq2seq.py` — wrapper that runs encoder and decoder and handles teacher forcing\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Notation and assumptions (explicit)\n\n- `e` = embedding dimension for characters\n- `h` = hidden dimension for encoder and decoder (per direction)\n- `T` = input sequence length (we assume input and output lengths equal for derivation)\n- `V` = vocabulary size (same for source and target in this derivation)\n- Single encoder layer and single decoder layer unless otherwise noted\n- Vanilla RNN cell by default; I provide LSTM/GRU adjustments later\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Parameter count — symbolic derivation (vanilla RNN)\n\nComponents:\n- Source embedding: `V * e`\n- Target embedding: `V * e`\n- Encoder RNN (1 layer): `e*h + h^2 + h`  (input-to-hidden, hidden-to-hidden, bias)\n- Decoder RNN (1 layer): `e*h + h^2 + h`\n- Output linear: `h*V + V`\n\nTotal parameters (vanilla RNN):\n\n\\[ P_{total} = 2 V e + 2(eh + h^2 + h) + hV + V \\]\n\nNotes:\n- If you tie source and target embeddings, subtract one `Ve` term.\n- If you use weight-tying for output projection, you can reuse embedding transpose and remove `hV`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Computation count (dominant matrix multiplies) — symbolic\n\nPer encoder time-step (matmuls): `e*h + h^2`.\nPer decoder time-step (matmuls): `e*h + h^2 + h*V` (the `h*V` is output projection).\nTotal for T encoder and T decoder steps:\n\n\\[ \\text{Total matmuls} = T \\cdot (2 e h + 2 h^2 + h V) \\]\n\nIf you want multiply+add FLOPs, multiply matmul count by 2. For backprop (training), expect ~2–3× forward cost depending on operations; a rough training FLOP estimate is `≈3 × forward_FLOPs`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. LSTM / GRU adjustments (brief)\n- GRU roughly multiplies the RNN parameter term by 3 (reset, update, candidate): replace `(eh + h^2 + h)` with `3*(eh + h^2) + 3*h`.\n- LSTM multiplies it by 4 (input, forget, output, cell): replace with `4*(eh + h^2) + 4*h`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Worked numeric example (careful arithmetic)\n\nChoose: `e=128, h=256, V=5000, T=20`.\n\nParameter counts:\n- Source emb: `5000 * 128 = 640,000`\n- Target emb: `640,000`\n- Encoder RNN: `128*256 + 256*256 + 256 = 32,768 + 65,536 + 256 = 98,560`\n- Decoder RNN: `98,560`\n- Output proj: `256*5000 + 5000 = 1,280,000 + 5,000 = 1,285,000`\n\nTotal parameters:\n\n```\nP_total = 640,000 + 640,000 + 98,560 + 98,560 + 1,285,000\n        = 2,762,120 parameters (~2.76M)\n```\n\nMemory (float32): `~2.76M * 4 bytes ≈ 10.5 MB`.\n\nForward-pass matmuls:\n- `2 e h = 2 * 128 * 256 = 65,536`\n- `2 h^2 = 2 * 256 * 256 = 131,072`\n- `h V = 256 * 5000 = 1,280,000`\nSum inside parens: `1,476,608`.\nMultiply by `T=20` → `29,532,160` matmuls.\nMultiply-add FLOPs ≈ `59,064,320` FLOPs (forward).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. How to run training (quick test)\n\nBelow is a minimal code snippet you can run in Colab or locally to perform a quick smoke test on the tiny sample dataset included in `data/raw/sample.tsv`.\nThis *won't* execute automatically in the notebook until you run the cell.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%bash\npython -V\necho \"To run quick test:\"\necho \"python scripts/train.py --config config/model_config.yaml --quick_test\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Attention visualization (how-to)\n\nWhen you enable `use_attention=True` in the model config and run evaluation, `eval_epoch` collects sample predictions. To visualize attention, perform these steps in a cell:\n\n1. Run model in evaluation mode on a single example and capture attention weights returned by the decoder (we stored attention per step in the decoder).  \n2. Build a matrix of shape `(T_out, T_enc)` containing attention weights at every decoder step.  \n3. Plot with matplotlib's `imshow` and annotate with source/target characters.\n\nA short code sketch is included below (not executed here):\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Attention visualization sketch (run in notebook after training)\nimport json\nimport torch\nimport matplotlib.pyplot as plt\nfrom src.data.tokenizer import CharacterTokenizer\nfrom src.models.encoder import Encoder\nfrom src.models.decoder import Decoder\nfrom src.models.seq2seq import Seq2Seq\n\n# load tokenizers saved by train.py\nwith open('checkpoints/src_itos.json','r',encoding='utf-8') as f:\n    src_itos = json.load(f)\nwith open('checkpoints/tgt_itos.json','r',encoding='utf-8') as f:\n    tgt_itos = json.load(f)\n\nsrc_tok = CharacterTokenizer({c:i for i,c in enumerate(src_itos)}, src_itos)\ntgt_tok = CharacterTokenizer({c:i for i,c in enumerate(tgt_itos)}, tgt_itos)\n\n# create model (match config)\nenc = Encoder(input_dim=len(src_tok.itos), embed_dim=128, hidden_dim=256, rnn_type='LSTM')\ndec = Decoder(output_dim=len(tgt_tok.itos), embed_dim=128, hidden_dim=256, rnn_type='LSTM', use_attention=True)\nmodel = Seq2Seq(enc, dec, sos_idx=tgt_tok.sos, eos_idx=tgt_tok.eos, device='cpu')\n\n# load best checkpoint\nckpt = torch.load('checkpoints/best.pt', map_location='cpu')\nmodel.load_state_dict(ckpt['model_state'])\nmodel.eval()\n\n# example: single source\nsrc_text = 'ghar'\nsrc_ids = [src_tok.sos] + src_tok.encode(src_text) + [src_tok.eos]\nimport torch\nsrc_tensor = torch.tensor([src_ids], dtype=torch.long)\nsrc_lens = torch.tensor([len(src_ids)], dtype=torch.long)\n\n# run greedy inference step-by-step, collecting attention\nenc_outs, enc_hidden = model.encoder(src_tensor, src_lens)\nenc_mask = (torch.arange(enc_outs.size(1)).unsqueeze(0) < src_lens.unsqueeze(1))\n\ndec_hidden = enc_hidden\ninput_tok = torch.tensor([tgt_tok.sos], dtype=torch.long)\nattn_matrix = []\npreds = []\nfor t in range(20):\n    logits, dec_hidden, attn = model.decoder(input_tok, dec_hidden, encoder_outputs=enc_outs, encoder_mask=enc_mask)\n    pred = logits.argmax(dim=1).item()\n    preds.append(pred)\n    if attn is not None:\n        attn_matrix.append(attn.squeeze(0).numpy())\n    input_tok = torch.tensor([pred], dtype=torch.long)\n    if pred == tgt_tok.eos:\n        break\n\n# plot attention heatmap\nif len(attn_matrix) > 0:\n    import numpy as np\n    A = np.stack(attn_matrix)  # (T_out, T_enc)\n    plt.imshow(A, aspect='auto')\n    plt.xlabel('Source positions')\n    plt.ylabel('Decoder steps')\n    plt.title('Attention heatmap')\n    plt.colorbar()\n    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Conclusion & next steps\n\n- The codebase is modular and ready for experiments (add beam search, transformer baseline, or multilingual training).  \n- The notebook contains the complete math derivation and a numeric worked example suitable for the assignment submission.  \n- Next I can: run a quick test and paste logs here, add beam search, or produce attention heatmaps for a few examples.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}